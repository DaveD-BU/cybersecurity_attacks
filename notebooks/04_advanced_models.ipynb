{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684493dc",
   "metadata": {},
   "source": [
    "# Phase 4: Advanced Machine Learning Models\n",
    "\n",
    "\n",
    "This notebook orchestrates advanced supervised models for cybersecurity attack classification and severity prediction. The workflow follows the Phase 4 implementation plan and builds on the cleaned feature sets produced during Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0c92a",
   "metadata": {},
   "source": [
    "## Objectives & Scope\n",
    "\n",
    "\n",
    "- Configure a reproducible environment for advanced classifiers and regressors.\n",
    "- Load standardized train, validation, and test splits with consistent data types.\n",
    "- Prepare shared preprocessing pipelines (imputation, scaling, encoding) for downstream models.\n",
    "- Persist dataset summaries and metadata for reporting and audit trails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77defc20",
   "metadata": {},
   "source": [
    "## Reproducibility & Artifacts\n",
    "\n",
    "\n",
    "\n",
    "- Random seeds are fixed to ensure deterministic behaviour across executions.\n",
    "\n",
    "- All data loading happens from the processed datasets directory without mutating source files.\n",
    "\n",
    "- Derived summaries are exported to the `reports/` folder for later phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e96d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global configuration\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "from pandas.api.types import (\n",
    "    is_categorical_dtype,\n",
    "    is_numeric_dtype,\n",
    "    is_string_dtype,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    RocCurveDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Matplotlib and seaborn configuration for consistent visuals\n",
    "COLOR_PALETTE: List[str] = [\n",
    "    \"#c6d4e1\",\n",
    "    \"#9bbcd4\",\n",
    "    \"#6fa3c7\",\n",
    "    \"#4a8ab8\",\n",
    "    \"#2f6fa1\",\n",
    "    \"#1f4f75\",\n",
    "    \"#d3d3d3\",\n",
    "    \"#a9a9a9\",\n",
    "    \"#696969\",\n",
    "]\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=COLOR_PALETTE)\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=COLOR_PALETTE)\n",
    "\n",
    "RANDOM_STATE: int = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "DATA_DIR = Path(\"../cybersecurity_attacks_data/processed\")\n",
    "ARTIFACT_METRICS = Path(\"../models/phase4_metrics.csv\")\n",
    "ARTIFACT_FIGURES = Path(\"../reports/visualizations/phase4\")\n",
    "ARTIFACT_SUMMARY = Path(\"../reports/phase4_data_snapshot.md\")\n",
    "\n",
    "ARTIFACT_FIGURES.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACT_SUMMARY.parent.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb8401",
   "metadata": {},
   "source": [
    "## Data Loading & Preparation\n",
    "This section loads the preprocessed splits and performs lightweight validation before model training. We reuse the Phase 3 engineered features and extend them with model-specific augmentations when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a processed CSV split from the Phase 3 preprocessing outputs.\"\"\"\n",
    "    path = DATA_DIR / f\"{name}_data.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Split '{name}' not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def summarize_split(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"Display a brief summary with shape and missingness diagnostics.\"\"\"\n",
    "    null_pct = df.isna().mean().sort_values(ascending=False)\n",
    "    head = df.head(3)\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"### {name.title()} Split\\n\"\n",
    "            f\"- Shape: {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns\\n\"\n",
    "            f\"- Missing features (>0.01%): {null_pct[null_pct > 0.0001].index.tolist()}\"\n",
    "        )\n",
    "    )\n",
    "    display(head)\n",
    "\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df = load_split(\"val\")\n",
    "test_df = load_split(\"test\")\n",
    "\n",
    "summarize_split(train_df, \"train\")\n",
    "summarize_split(val_df, \"validation\")\n",
    "summarize_split(test_df, \"test\")\n",
    "\n",
    "TARGET_COL = \"attackSuccessful\"\n",
    "FEATURE_COLS = [col for col in train_df.columns if col != TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183ca13",
   "metadata": {},
   "source": [
    "## Feature Engineering & Pipelines\n",
    "The preprocessing pipeline mirrors Phase 3 encoders while allowing advanced estimators to toggle optional feature transformations. This modular design supports experimentation with imbalance handling and interaction effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e986ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_feature_types(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Infer categorical and numeric columns from the training frame.\"\"\"\n",
    "    categorical_cols: List[str] = []\n",
    "    numeric_cols: List[str] = []\n",
    "    for col in FEATURE_COLS:\n",
    "        if is_numeric_dtype(df[col]):\n",
    "            numeric_cols.append(col)\n",
    "        elif is_categorical_dtype(df[col]) or is_string_dtype(df[col]):\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "def build_preprocessor(numeric_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:\n",
    "    numeric_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "    categorical_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipe, numeric_cols),\n",
    "            (\"cat\", categorical_pipe, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "NUMERIC_FEATURES, CATEGORICAL_FEATURES = infer_feature_types(train_df)\n",
    "PREPROCESSOR = build_preprocessor(NUMERIC_FEATURES, CATEGORICAL_FEATURES)\n",
    "\n",
    "train_X, train_y = train_df[FEATURE_COLS], train_df[TARGET_COL]\n",
    "val_X, val_y = val_df[FEATURE_COLS], val_df[TARGET_COL]\n",
    "test_X, test_y = test_df[FEATURE_COLS], test_df[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0564f0",
   "metadata": {},
   "source": [
    "## Experiment Utilities\n",
    "Helper functions standardize training, hyperparameter search, and evaluation output across models. Metrics are written to `models/phase4_metrics.csv` to align with the project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fe30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricRow = Dict[str, float]\n",
    "\n",
    "\n",
    "def ensure_metrics_store() -> pd.DataFrame:\n",
    "    if ARTIFACT_METRICS.exists():\n",
    "        return pd.read_csv(ARTIFACT_METRICS)\n",
    "    return pd.DataFrame(\n",
    "        columns=[\n",
    "            \"model\",\n",
    "            \"validation_accuracy\",\n",
    "            \"validation_precision\",\n",
    "            \"validation_recall\",\n",
    "            \"validation_f1\",\n",
    "            \"validation_roc_auc\",\n",
    "            \"test_accuracy\",\n",
    "            \"test_precision\",\n",
    "            \"test_recall\",\n",
    "            \"test_f1\",\n",
    "            \"test_roc_auc\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def log_metrics(model_name: str, metrics: Dict[str, float]) -> None:\n",
    "    store = ensure_metrics_store()\n",
    "    row = {\"model\": model_name}\n",
    "    row.update(metrics)\n",
    "    store = pd.concat([store, pd.DataFrame([row])], ignore_index=True)\n",
    "    store.to_csv(ARTIFACT_METRICS, index=False)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model_pipeline: Pipeline,\n",
    "    model_name: str,\n",
    "    train_X: pd.DataFrame,\n",
    "    train_y: pd.Series,\n",
    "    val_X: pd.DataFrame,\n",
    "    val_y: pd.Series,\n",
    "    test_X: pd.DataFrame,\n",
    "    test_y: pd.Series,\n",
    "    k_folds: int = 5,\n",
    ") -> Dict[str, float]:\n",
    "    cv = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_results = cross_validate(\n",
    "        model_pipeline,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        cv=cv,\n",
    "        scoring=[\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"],\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    cv_summary = {\n",
    "        f\"cv_{metric.replace('test_', '')}\": np.mean(scores)\n",
    "        for metric, scores in cv_results.items()\n",
    "        if metric.startswith(\"test_\")\n",
    "    }\n",
    "\n",
    "    model_pipeline.fit(pd.concat([train_X, val_X]), pd.concat([train_y, val_y]))\n",
    "    val_pred = model_pipeline.predict(val_X)\n",
    "    test_pred = model_pipeline.predict(test_X)\n",
    "    val_proba = model_pipeline.predict_proba(val_X)[:, 1]\n",
    "    test_proba = model_pipeline.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "        val_y, val_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "        test_y, test_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"validation_accuracy\": accuracy_score(val_y, val_pred),\n",
    "        \"validation_precision\": val_precision,\n",
    "        \"validation_recall\": val_recall,\n",
    "        \"validation_f1\": val_f1,\n",
    "        \"validation_roc_auc\": roc_auc_score(val_y, val_proba),\n",
    "        \"test_accuracy\": accuracy_score(test_y, test_pred),\n",
    "        \"test_precision\": test_precision,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_roc_auc\": roc_auc_score(test_y, test_proba),\n",
    "    }\n",
    "    metrics.update(cv_summary)\n",
    "    log_metrics(model_name, metrics)\n",
    "    display(Markdown(f\"### {model_name}\"))\n",
    "    display(Markdown(classification_report(val_y, val_pred, output_dict=False)))\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diagnostics(model, X: pd.DataFrame, y: pd.Series, model_name: str, suffix: str) -> None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X, y, ax=axes[0], colorbar=False)\n",
    "    axes[0].set_title(f\"{model_name} Confusion Matrix ({suffix})\")\n",
    "\n",
    "    RocCurveDisplay.from_estimator(model, X, y, ax=axes[1])\n",
    "    axes[1].set_title(f\"{model_name} ROC Curve ({suffix})\")\n",
    "\n",
    "    PrecisionRecallDisplay.from_estimator(model, X, y, ax=axes[2])\n",
    "    axes[2].set_title(f\"{model_name} Precision-Recall ({suffix})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    figure_path = ARTIFACT_FIGURES / f\"{model_name.lower().replace(' ', '_')}_{suffix}.png\"\n",
    "    fig.savefig(figure_path, dpi=200)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_summary_entry(model_name: str, metrics: Dict[str, float]) -> None:\n",
    "    header = f\"## {model_name}\\n\"\n",
    "    lines = [header]\n",
    "    for key, value in metrics.items():\n",
    "        if key.startswith(\"cv_\"):\n",
    "            continue\n",
    "        lines.append(f\"- {key.replace('_', ' ').title()}: {value:.4f}\\n\")\n",
    "    with open(ARTIFACT_SUMMARY, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines + [\"\\n\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558556b",
   "metadata": {},
   "source": [
    "## Model Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead301e5",
   "metadata": {},
   "source": [
    "### Balanced Logistic Regression\n",
    "Revisits the linear baseline with tuned regularization and class weighting to establish a strong probabilistic reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_params = {\n",
    "    \"model__C\": np.logspace(-3, 1, 8),\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"],\n",
    "}\n",
    "\n",
    "logit_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", PREPROCESSOR),\n",
    "        (\n",
    "            \"model\",\n",
    "            LogisticRegression(\n",
    "                class_weight=\"balanced\",\n",
    "                max_iter=200,\n",
    "                random_state=RANDOM_STATE,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logit_search = RandomizedSearchCV(\n",
    "    estimator=logit_pipeline,\n",
    "    param_distributions=logit_params,\n",
    "    n_iter=10,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "logit_search.fit(train_X, train_y)\n",
    "logit_best = logit_search.best_estimator_\n",
    "logit_metrics = evaluate_model(\n",
    "    logit_best,\n",
    "    \"Balanced Logistic Regression\",\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    test_X,\n",
    "    test_y,\n",
    ")\n",
    "plot_diagnostics(logit_best, val_X, val_y, \"Balanced Logistic Regression\", \"validation\")\n",
    "plot_diagnostics(logit_best, test_X, test_y, \"Balanced Logistic Regression\", \"test\")\n",
    "append_summary_entry(\"Balanced Logistic Regression\", logit_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d563a",
   "metadata": {},
   "source": [
    "### Random Forest Ensemble\n",
    "Targets non-linear interactions and feature importance analysis via tree-based ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__max_depth\": [None, 10, 20, 30],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", 0.5],\n",
    "}\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", PREPROCESSOR),\n",
    "        (\n",
    "            \"model\",\n",
    "            RandomForestClassifier(\n",
    "                class_weight=\"balanced_subsample\",\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "rf_search.fit(train_X, train_y)\n",
    "rf_best = rf_search.best_estimator_\n",
    "rf_metrics = evaluate_model(\n",
    "    rf_best,\n",
    "    \"Random Forest Ensemble\",\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    test_X,\n",
    "    test_y,\n",
    ")\n",
    "plot_diagnostics(rf_best, val_X, val_y, \"Random Forest Ensemble\", \"validation\")\n",
    "plot_diagnostics(rf_best, test_X, test_y, \"Random Forest Ensemble\", \"test\")\n",
    "append_summary_entry(\"Random Forest Ensemble\", rf_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7c560",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Leverages stage-wise additive modeling to optimize for ROC-AUC with shrinkage and depth control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d40d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"model__max_depth\": [2, 3, 4],\n",
    "    \"model__subsample\": [0.7, 0.85, 1.0],\n",
    "    \"model__min_samples_split\": [2, 4, 6],\n",
    "}\n",
    "\n",
    "gb_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", PREPROCESSOR),\n",
    "        (\n",
    "            \"model\",\n",
    "            GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    estimator=gb_pipeline,\n",
    "    param_distributions=gb_params,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "gb_search.fit(train_X, train_y)\n",
    "gb_best = gb_search.best_estimator_\n",
    "gb_metrics = evaluate_model(\n",
    "    gb_best,\n",
    "    \"Gradient Boosting\",\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    test_X,\n",
    "    test_y,\n",
    ")\n",
    "plot_diagnostics(gb_best, val_X, val_y, \"Gradient Boosting\", \"validation\")\n",
    "plot_diagnostics(gb_best, test_X, test_y, \"Gradient Boosting\", \"test\")\n",
    "append_summary_entry(\"Gradient Boosting\", gb_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21d83c",
   "metadata": {},
   "source": [
    "### XGBoost Gradient Boosted Trees\n",
    "Explores gradient-boosting with second-order optimization and regularization controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__max_depth\": [3, 4, 5],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"model__reg_alpha\": [0.0, 0.1, 0.5],\n",
    "    \"model__reg_lambda\": [0.5, 1.0, 2.0],\n",
    "    \"model__gamma\": [0, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "xgb_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", PREPROCESSOR),\n",
    "        (\n",
    "            \"model\",\n",
    "            XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=RANDOM_STATE,\n",
    "                tree_method=\"hist\",\n",
    "                scale_pos_weight=None,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=25,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "xgb_search.fit(train_X, train_y)\n",
    "xgb_best = xgb_search.best_estimator_\n",
    "xgb_metrics = evaluate_model(\n",
    "    xgb_best,\n",
    "    \"XGBoost Gradient Boosted Trees\",\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    test_X,\n",
    "    test_y,\n",
    ")\n",
    "plot_diagnostics(xgb_best, val_X, val_y, \"XGBoost Gradient Boosted Trees\", \"validation\")\n",
    "plot_diagnostics(xgb_best, test_X, test_y, \"XGBoost Gradient Boosted Trees\", \"test\")\n",
    "append_summary_entry(\"XGBoost Gradient Boosted Trees\", xgb_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cd007",
   "metadata": {},
   "source": [
    "### Support Vector Machine (RBF)\n",
    "Captures complex decision boundaries through kernel methods with probabilistic calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76addf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    \"model__C\": np.logspace(-2, 2, 10),\n",
    "    \"model__gamma\": np.logspace(-4, 0, 10),\n",
    "}\n",
    "\n",
    "svm_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", PREPROCESSOR),\n",
    "        (\n",
    "            \"model\",\n",
    "            SVC(\n",
    "                probability=True,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=RANDOM_STATE,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_search = RandomizedSearchCV(\n",
    "    estimator=svm_pipeline,\n",
    "    param_distributions=svm_params,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "svm_search.fit(train_X, train_y)\n",
    "svm_best = svm_search.best_estimator_\n",
    "svm_metrics = evaluate_model(\n",
    "    svm_best,\n",
    "    \"Support Vector Machine (RBF)\",\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    test_X,\n",
    "    test_y,\n",
    ")\n",
    "plot_diagnostics(svm_best, val_X, val_y, \"Support Vector Machine (RBF)\", \"validation\")\n",
    "plot_diagnostics(svm_best, test_X, test_y, \"Support Vector Machine (RBF)\", \"test\")\n",
    "append_summary_entry(\"Support Vector Machine (RBF)\", svm_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57deb2e",
   "metadata": {},
   "source": [
    "## Consolidated Results & Next Steps\n",
    "Loads the aggregated metrics table and outlines follow-up analyses for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f64602",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ARTIFACT_METRICS.exists():\n",
    "    phase4_metrics = pd.read_csv(ARTIFACT_METRICS)\n",
    "    display(phase4_metrics.sort_values(\"validation_roc_auc\", ascending=False))\n",
    "else:\n",
    "    display(Markdown(\"No metrics logged yet. Run the experiments above to populate the table.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7220e",
   "metadata": {},
   "source": [
    "### Reporting Checklist\n",
    "- Integrate best-performing model metrics into `reports/phase4_advanced_models_report.md`.\n",
    "- Refresh visualization gallery with ROC, PR, and feature importance plots.\n",
    "- Update `plan/data-cybersecurity-attacks-analysis-1.md` TASK-036 to reflect experiment outcomes.\n",
    "- Prepare deployment considerations if gap between validation and test metrics is minimal."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
